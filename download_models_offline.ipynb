{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Hugging Face Models for Offline Use\n",
    "\n",
    "This notebook downloads Hugging Face models to a configurable location for offline use in production environments.\n",
    "\n",
    "## Configuration\n",
    "Set your desired cache directory and models to download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.12 | packaged by conda-forge | (main, Apr 10 2025, 22:18:52) [Clang 18.1.8 ]\n",
      "PyTorch version: 2.1.0\n",
      "Transformers available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    LayoutLMForTokenClassification,\n",
    "    LayoutLMTokenizer,\n",
    "    LayoutLMv2ForTokenClassification,\n",
    "    LayoutLMv2Tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers available: {True}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "Configure the download location and models to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cache directory: /Users/tod/PretrainedLLM\n",
      "üîÑ Force redownload: False\n",
      "üì¶ Models to download: 1\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify these as needed\n",
    "CACHE_DIR = os.getenv(\"HF_HOME\", \"/Users/tod/PretrainedLLM\")\n",
    "FORCE_REDOWNLOAD = False  # Set to True to re-download existing models\n",
    "\n",
    "# Models to download - Add or remove as needed\n",
    "MODELS_TO_DOWNLOAD = [\n",
    "    {\n",
    "        \"name\": \"microsoft/layoutlm-base-uncased\",\n",
    "        \"type\": \"layoutlm\",\n",
    "        \"description\": \"LayoutLM base model for document understanding\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìÅ Cache directory: {CACHE_DIR}\")\n",
    "print(f\"üîÑ Force redownload: {FORCE_REDOWNLOAD}\")\n",
    "print(f\"üì¶ Models to download: {len(MODELS_TO_DOWNLOAD)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cache Directory\n",
    "\n",
    "Create the cache directory and set environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache directory created: /Users/tod/PretrainedLLM\n",
      "üåç Environment variables set:\n",
      "  HF_HOME: /Users/tod/PretrainedLLM\n",
      "  TRANSFORMERS_CACHE: /Users/tod/PretrainedLLM\n",
      "  HF_DATASETS_CACHE: /Users/tod/PretrainedLLM/datasets\n"
     ]
    }
   ],
   "source": [
    "# Create cache directory\n",
    "cache_path = Path(CACHE_DIR)\n",
    "cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set environment variables for Hugging Face\n",
    "os.environ[\"HF_HOME\"] = str(cache_path)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(cache_path)\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(cache_path / \"datasets\")\n",
    "\n",
    "print(f\"‚úÖ Cache directory created: {cache_path}\")\n",
    "print(\"üåç Environment variables set:\")\n",
    "print(f\"  HF_HOME: {os.environ['HF_HOME']}\")\n",
    "print(f\"  TRANSFORMERS_CACHE: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "print(f\"  HF_DATASETS_CACHE: {os.environ['HF_DATASETS_CACHE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Helper Functions\n",
    "\n",
    "Functions to download different types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layoutlm_model(model_name, cache_dir, force_redownload=False):\n",
    "    \"\"\"Download LayoutLM model and tokenizer.\"\"\"\n",
    "    print(f\"üì• Downloading LayoutLM: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Download tokenizer\n",
    "        print(\"  üìù Downloading tokenizer...\")\n",
    "        _tokenizer = LayoutLMTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        # Download model\n",
    "        print(\"  üß† Downloading model...\")\n",
    "        _model = LayoutLMForTokenClassification.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        print(\"  ‚úÖ LayoutLM downloaded successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error downloading LayoutLM: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_layoutlmv2_model(model_name, cache_dir, force_redownload=False):\n",
    "    \"\"\"Download LayoutLMv2 model and tokenizer.\"\"\"\n",
    "    print(f\"üì• Downloading LayoutLMv2: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Download tokenizer\n",
    "        print(\"  üìù Downloading tokenizer...\")\n",
    "        _tokenizer = LayoutLMv2Tokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        # Download model\n",
    "        print(\"  üß† Downloading model...\")\n",
    "        _model = LayoutLMv2ForTokenClassification.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        print(\"  ‚úÖ LayoutLMv2 downloaded successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error downloading LayoutLMv2: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_auto_model(model_name, cache_dir, force_redownload=False):\n",
    "    \"\"\"Download any model using AutoModel and AutoTokenizer.\"\"\"\n",
    "    print(f\"üì• Downloading Auto model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Download tokenizer\n",
    "        print(\"  üìù Downloading tokenizer...\")\n",
    "        _tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        # Download model\n",
    "        print(\"  üß† Downloading model...\")\n",
    "        _model = AutoModel.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, force_download=force_redownload\n",
    "        )\n",
    "\n",
    "        print(\"  ‚úÖ Auto model downloaded successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error downloading Auto model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_model_downloader(model_type):\n",
    "    \"\"\"Get the appropriate download function for model type.\"\"\"\n",
    "    downloaders = {\n",
    "        \"layoutlm\": download_layoutlm_model,\n",
    "        \"layoutlmv2\": download_layoutlmv2_model,\n",
    "        \"bert\": download_auto_model,\n",
    "        \"auto\": download_auto_model,\n",
    "    }\n",
    "    return downloaders.get(model_type, download_auto_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models\n",
    "\n",
    "Download all configured models to the cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting download of 1 models...\n",
      "\n",
      "üì¶ [1/1] microsoft/layoutlm-base-uncased\n",
      "   üìã LayoutLM base model for document understanding\n",
      "   üè∑Ô∏è  Type: layoutlm\n",
      "üì• Downloading LayoutLM: microsoft/layoutlm-base-uncased\n",
      "  üìù Downloading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7911afde0130461e87cc1887aca5c42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7688e70c1af348adb60f019c4dc76c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d3b6cd53c144da8650d7accad80451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2041e378c2b141fdb1ecc9b1d5b9973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f56811d4f284753b9d7ac562e6ea89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üß† Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819308a5dc8c4803a2af19984edd06e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ LayoutLM downloaded successfully\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìä DOWNLOAD SUMMARY\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "‚úÖ Successfully downloaded: 1\n",
      "   ‚úì microsoft/layoutlm-base-uncased\n",
      "\n",
      "üéâ All models downloaded successfully!\n",
      "\n",
      "üìÅ Models cached in: /Users/tod/PretrainedLLM\n"
     ]
    }
   ],
   "source": [
    "# Download all models\n",
    "print(f\"üöÄ Starting download of {len(MODELS_TO_DOWNLOAD)} models...\\n\")\n",
    "\n",
    "downloaded_successfully = []\n",
    "failed_downloads = []\n",
    "\n",
    "for i, model_config in enumerate(MODELS_TO_DOWNLOAD, 1):\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_type = model_config[\"type\"]\n",
    "    description = model_config[\"description\"]\n",
    "\n",
    "    print(f\"üì¶ [{i}/{len(MODELS_TO_DOWNLOAD)}] {model_name}\")\n",
    "    print(f\"   üìã {description}\")\n",
    "    print(f\"   üè∑Ô∏è  Type: {model_type}\")\n",
    "\n",
    "    # Get the appropriate downloader\n",
    "    downloader = get_model_downloader(model_type)\n",
    "\n",
    "    # Download the model\n",
    "    success = downloader(model_name, CACHE_DIR, FORCE_REDOWNLOAD)\n",
    "\n",
    "    if success:\n",
    "        downloaded_successfully.append(model_name)\n",
    "    else:\n",
    "        failed_downloads.append(model_name)\n",
    "\n",
    "    print()  # Empty line for readability\n",
    "\n",
    "# Summary\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"üìä DOWNLOAD SUMMARY\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(f\"‚úÖ Successfully downloaded: {len(downloaded_successfully)}\")\n",
    "for model in downloaded_successfully:\n",
    "    print(f\"   ‚úì {model}\")\n",
    "\n",
    "if failed_downloads:\n",
    "    print(f\"\\n‚ùå Failed downloads: {len(failed_downloads)}\")\n",
    "    for model in failed_downloads:\n",
    "        print(f\"   ‚úó {model}\")\n",
    "else:\n",
    "    print(\"\\nüéâ All models downloaded successfully!\")\n",
    "\n",
    "print(f\"\\nüìÅ Models cached in: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads\n",
    "\n",
    "Check what was actually downloaded to the cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cache Directory Contents:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìÑ .DS_Store (0.0 MB)\n",
      "üìÅ .locks/ (7 files)\n",
      "üìÅ InternVL2_5-1B/ (74 files)\n",
      "üìÅ InternVL3-1B/ (67 files)\n",
      "üìÅ Llama-3.2-1B/ (10 files)\n",
      "üìÅ ModernBERT-base/ (5 files)\n",
      "üìÅ all-MiniLM-L12-v2/ (18 files)\n",
      "üìÅ all-MiniLM-L6-v2/ (6 files)\n",
      "üìÅ all-mpnet-base-v2/ (14 files)\n",
      "üìÅ gte-base-en-v1.5/ (12 files)\n",
      "üìÅ gte-small/ (14 files)\n",
      "üìÑ hf_llama_token.txt (0.0 MB)\n",
      "üìÅ models--microsoft--layoutlm-base-uncased/ (21 files)\n",
      "üìÅ paraphrase-MiniLM-L3-v2/ (12 files)\n",
      "üìÅ paraphrase-MiniLM-L6-v2/ (12 files)\n",
      "üìÅ paraphrase-multilingual-MiniLM-L12-v2/ (95 files)\n",
      "üìÅ roberta_model/ (7 files)\n",
      "üìÅ swin_large/ (3 files)\n",
      "\n",
      "üìä Cache Statistics:\n",
      "üíæ Total size: 7796.9 MB (7.61 GB)\n",
      "üìÅ Cache location: /Users/tod/PretrainedLLM\n"
     ]
    }
   ],
   "source": [
    "# List contents of cache directory\n",
    "print(\"üìÇ Cache Directory Contents:\")\n",
    "print(\"‚ïê\" * 40)\n",
    "\n",
    "cache_path = Path(CACHE_DIR)\n",
    "if cache_path.exists():\n",
    "    for item in sorted(cache_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            # Count files in subdirectory\n",
    "            try:\n",
    "                file_count = len(list(item.rglob(\"*\")))\n",
    "                print(f\"üìÅ {item.name}/ ({file_count} files)\")\n",
    "            except PermissionError:\n",
    "                print(f\"üìÅ {item.name}/ (permission denied)\")\n",
    "        else:\n",
    "            file_size = item.stat().st_size / (1024 * 1024)  # MB\n",
    "            print(f\"üìÑ {item.name} ({file_size:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå Cache directory does not exist\")\n",
    "\n",
    "# Show total cache size\n",
    "try:\n",
    "    total_size = sum(f.stat().st_size for f in cache_path.rglob(\"*\") if f.is_file())\n",
    "    total_size_mb = total_size / (1024 * 1024)\n",
    "    total_size_gb = total_size_mb / 1024\n",
    "\n",
    "    print(\"\\nüìä Cache Statistics:\")\n",
    "    print(f\"üíæ Total size: {total_size_mb:.1f} MB ({total_size_gb:.2f} GB)\")\n",
    "    print(f\"üìÅ Cache location: {cache_path.absolute()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not calculate cache size: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Offline Loading\n",
    "\n",
    "Test that downloaded models can be loaded offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Offline Model Loading\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üîç Testing: microsoft/layoutlm-base-uncased\n",
      "  ‚úÖ Loaded successfully offline\n",
      "\n",
      "üìã Offline Loading Test Results:\n",
      "‚úÖ Successfully loaded offline: 1/1\n",
      "  ‚úÖ microsoft/layoutlm-base-uncased\n",
      "\n",
      "üéâ All models ready for offline use!\n"
     ]
    }
   ],
   "source": [
    "# Test offline loading of downloaded models\n",
    "print(\"üß™ Testing Offline Model Loading\")\n",
    "print(\"‚ïê\" * 40)\n",
    "\n",
    "# Simulate offline environment\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_config in MODELS_TO_DOWNLOAD:\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_type = model_config[\"type\"]\n",
    "\n",
    "    print(f\"üîç Testing: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        if model_type == \"layoutlm\":\n",
    "            tokenizer = LayoutLMTokenizer.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "            model = LayoutLMForTokenClassification.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "        elif model_type == \"layoutlmv2\":\n",
    "            tokenizer = LayoutLMv2Tokenizer.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "            model = LayoutLMv2ForTokenClassification.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name, local_files_only=True, cache_dir=CACHE_DIR\n",
    "            )\n",
    "\n",
    "        print(\"  ‚úÖ Loaded successfully offline\")\n",
    "        test_results.append((model_name, True))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to load offline: {e}\")\n",
    "        test_results.append((model_name, False))\n",
    "\n",
    "# Summary of offline tests\n",
    "print(\"\\nüìã Offline Loading Test Results:\")\n",
    "successful_offline = sum(1 for _, success in test_results if success)\n",
    "print(f\"‚úÖ Successfully loaded offline: {successful_offline}/{len(test_results)}\")\n",
    "\n",
    "for model_name, success in test_results:\n",
    "    status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "    print(f\"  {status} {model_name}\")\n",
    "\n",
    "if successful_offline == len(test_results):\n",
    "    print(\"\\nüéâ All models ready for offline use!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some models may need to be re-downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Generate environment variable settings for your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Environment Configuration\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Add these to your .env file or export them in your shell:\n",
      "\n",
      "\n",
      "# Hugging Face Cache Configuration\n",
      "export HF_HOME=\"/Users/tod/PretrainedLLM\"\n",
      "export TRANSFORMERS_CACHE=\"/Users/tod/PretrainedLLM\"\n",
      "export HF_DATASETS_CACHE=\"/Users/tod/PretrainedLLM/datasets\"\n",
      "\n",
      "# Offline mode (uncomment for production)\n",
      "# export HF_HUB_OFFLINE=1\n",
      "# export TRANSFORMERS_OFFLINE=1\n",
      "\n",
      "\n",
      "üìÑ .env file format:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "HF_HOME=/Users/tod/PretrainedLLM\n",
      "TRANSFORMERS_CACHE=/Users/tod/PretrainedLLM\n",
      "HF_DATASETS_CACHE=/Users/tod/PretrainedLLM/datasets\n",
      "# HF_HUB_OFFLINE=1\n",
      "# TRANSFORMERS_OFFLINE=1\n",
      "\n",
      "‚úÖ Models are ready for offline use from: /Users/tod/PretrainedLLM\n"
     ]
    }
   ],
   "source": [
    "# Generate environment configuration\n",
    "print(\"‚öôÔ∏è  Environment Configuration\")\n",
    "print(\"‚ïê\" * 40)\n",
    "print(\"Add these to your .env file or export them in your shell:\\n\")\n",
    "\n",
    "env_config = f\"\"\"\n",
    "# Hugging Face Cache Configuration\n",
    "export HF_HOME=\"{CACHE_DIR}\"\n",
    "export TRANSFORMERS_CACHE=\"{CACHE_DIR}\"\n",
    "export HF_DATASETS_CACHE=\"{CACHE_DIR}/datasets\"\n",
    "\n",
    "# Offline mode (uncomment for production)\n",
    "# export HF_HUB_OFFLINE=1\n",
    "# export TRANSFORMERS_OFFLINE=1\n",
    "\"\"\"\n",
    "\n",
    "print(env_config)\n",
    "\n",
    "# Also show .env file format\n",
    "print(\"\\nüìÑ .env file format:\")\n",
    "print(\"‚îÄ\" * 20)\n",
    "print(f\"HF_HOME={CACHE_DIR}\")\n",
    "print(f\"TRANSFORMERS_CACHE={CACHE_DIR}\")\n",
    "print(f\"HF_DATASETS_CACHE={CACHE_DIR}/datasets\")\n",
    "print(\"# HF_HUB_OFFLINE=1\")\n",
    "print(\"# TRANSFORMERS_OFFLINE=1\")\n",
    "\n",
    "print(f\"\\n‚úÖ Models are ready for offline use from: {CACHE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}